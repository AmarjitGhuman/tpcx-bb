{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to existing Dask CUDA Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "cluster_ip = 'YOUR_DASK_SCHEDULER_IP'\n",
    "client = Client(f'ucx://{cluster_ip}:8786')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "\n",
    "tpcx_bb_home = 'YOUR_TPCX_BB_REPO_LOCATION'\n",
    "spark_schema_dir = f'{tpcx_bb_home}/tpcx_bb/spark_table_schemas/'\n",
    "\n",
    "# Spark uses different names for column types, and RAPIDS doesn't yet support Decimal types.\n",
    "def get_schema(table):\n",
    "  with open(f'{spark_schema_dir}{table}.schema') as fp:\n",
    "    schema = fp.read()\n",
    "    names = [line.replace(',', '').split()[0] for line in schema.split('\\n')]\n",
    "    types = [line.replace(',', '').split()[1].replace('bigint', 'int').replace('string', 'str') for line in schema.split('\\n')]\n",
    "    types = [col_type.split('(')[0].replace('decimal', 'float') for col_type in types]\n",
    "    return names, types\n",
    "\n",
    "def read_csv_table(table, chunksize='256 MiB'):\n",
    "    # build dict of dtypes to use when reading CSV\n",
    "    names, types = get_schema(table)\n",
    "    dtype = {names[i]: types[i] for i in range(0, len(names))}\n",
    "\n",
    "    base_path = f'{data_dir}/data/{table}'\n",
    "    files = os.listdir(base_path)\n",
    "    # item_marketprices has \"audit\" files that should be excluded\n",
    "    if table == 'item_marketprices':\n",
    "        paths = [f'{base_path}/{fn}' for fn in files if 'audit' not in fn and os.path.getsize(f'{base_path}/{fn}') > 0]\n",
    "        base_path = f'{data_dir}/data_refresh/{table}'\n",
    "        paths = paths + [f'{base_path}/{fn}' for fn in os.listdir(base_path) if 'audit' not in fn and os.path.getsize(f'{base_path}/{fn}') > 0]\n",
    "        df = dask_cudf.read_csv(paths, sep='|', names=names, dtype=dtype, chunksize=chunksize, quoting=3)\n",
    "    else:\n",
    "        paths = [f'{base_path}/{fn}' for fn in files if os.path.getsize(f'{base_path}/{fn}') > 0]\n",
    "        if table in refresh_tables:\n",
    "            base_path = f'{data_dir}/data_refresh/{table}'\n",
    "            paths = paths + [f'{base_path}/{fn}' for fn in os.listdir(base_path) if os.path.getsize(f'{base_path}/{fn}') > 0]\n",
    "        df = dask_cudf.read_csv(paths, sep='|', names=names, dtype=types, chunksize=chunksize, quoting=3)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, math\n",
    "\n",
    "\n",
    "def multiplier(unit):\n",
    "    if unit == 'G':\n",
    "        return 1\n",
    "    elif unit == 'T':\n",
    "        return 1000\n",
    "    else: return 0\n",
    "\n",
    "# we use size of the CSV data on disk to determine number of Parquet partitions\n",
    "def get_size_gb(table):\n",
    "    path = data_dir + 'data/'+table\n",
    "    size = subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8')\n",
    "    unit = size[-1]\n",
    "    \n",
    "    size = math.ceil(float(size[:-1])) * multiplier(unit)\n",
    "    \n",
    "    if table in refresh_tables:\n",
    "        path = data_dir + 'data_refresh/'+table\n",
    "        refresh_size = subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8')\n",
    "        size = size + math.ceil(float(refresh_size[:-1])) * multiplier(refresh_size[-1])\n",
    "    \n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repartition(table, outdir, npartitions=None, chunksize=None, compression='snappy'):\n",
    "    size = get_size_gb(table)\n",
    "    if npartitions is None:\n",
    "        npartitions = max(1, size)\n",
    "    print(f'Converting {table} of {size} GB to {npartitions} parquet files, chunksize: {chunksize}')\n",
    "    read_csv_table(table, chunksize).repartition(npartitions=npartitions).to_parquet(outdir+table, compression=compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate list of tables to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# these tables have extra data produced by bigbench dataGen\n",
    "refresh_tables = [\n",
    "  'customer', 'customer_address',\n",
    "  'inventory', 'item', 'item_marketprices',\n",
    "  'product_reviews', 'store_returns', 'store_sales',\n",
    "  'web_clickstreams', 'web_returns', 'web_sales'\n",
    "]\n",
    "tables = [table.split('.')[0] for table in os.listdir(spark_schema_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all tables to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "scale = 'sf10000'\n",
    "part_size = 3\n",
    "chunksize = '128 MiB'\n",
    "\n",
    "# location of bigBench dataGen's CSV output\n",
    "data_dir = f'/mnt/weka/tpcx-bb/{scale}/'\n",
    "# location you want to write Parquet versions of the table data \n",
    "outdir = f'/mnt/weka/tpcx-bb/{scale}/parquet_{part_size}gb/'\n",
    "\n",
    "total = 0\n",
    "for table in tables:\n",
    "    size_gb = get_size_gb(table)\n",
    "    # product_reviews has lengthy strings which exceed cudf's max number of characters per column\n",
    "    # we use smaller partitions to avoid overflowing this character limit\n",
    "    if table == 'product_reviews':\n",
    "        npartitions = max(1, int(size_gb/1))\n",
    "    else:\n",
    "        npartitions = max(1, int(size_gb/part_size))\n",
    "    t0 = time.time()\n",
    "    repartition(table, outdir, npartitions, chunksize, compression='snappy')\n",
    "    t1 = time.time()\n",
    "    total = total + (t1-t0)\n",
    "    print(f'{table} took {t1-t0} of {total}\\n')\n",
    "print(f'{chunksize} took {total}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
